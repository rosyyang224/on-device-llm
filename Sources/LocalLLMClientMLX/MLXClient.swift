import LocalLLMClientCore
import MLX
import MLXLMCommon
import Foundation

/// A client for interacting with MLX models.
///
/// This actor-based class provides methods for generating text streams from various inputs,
/// and handles the communication with the underlying MLX model via the `MLX` and `MLXLMCommon` frameworks.
public final actor MLXClient: LLMClient {
    let context: Context
    let parameter: MLXClient.Parameter
    let tools: [AnyLLMTool]

    /// Initializes a new MLX client.
    ///
    /// - Parameters:
    ///   - url: The URL of the MLX model directory. This directory should contain the model weights, tokenizer configuration, and any other necessary model files.
    ///   - parameter: The parameters for the MLX model. Defaults to `.default`.
    ///   - tools: Optional array of tools that can be used by the model for function calling.
    /// - Throws: An error if the client fails to initialize, for example, if the model files cannot be loaded.
    nonisolated public init(url: URL, parameter: Parameter = .default, tools: [any LLMTool] = []) async throws {
        context = try await Context(url: url, parameter: parameter)
        self.parameter = parameter
        self.tools = tools.map { AnyLLMTool($0) }
    }

    /// Generates a text stream from the given input.
    ///
    /// This function processes the input, whether it's plain text, a chat template, or structured chat messages,
    /// and prepares it for the MLX model. It then generates text asynchronously.
    ///
    /// - Parameter input: The input to generate text from. This can be plain text, a chat template, or an array of chat messages.
    /// - Returns: An `AsyncStream<String>` that yields text chunks as they are generated by the model.
    /// - Throws: An `LLMError.visionUnsupported` error if the input contains images and the loaded model does not support vision.
    ///           It can also throw errors related to model processing or input preparation.
    public func textStream(from input: LLMInput) async throws -> AsyncStream<String> {
        let chat = input.chatMessages

        var userInput = UserInput(chat: chat, additionalContext: ["enable_thinking": false]) // TODO: public API
        userInput.processing.resize = .init(width: 448, height: 448)

        if chat.contains(where: { !$0.images.isEmpty }), !context.supportsVision {
            throw LLMError.visionUnsupported
        }
        let modelContainer =  context.modelContainer

        return try await modelContainer.perform { [userInput] context in
            let lmInput = try await context.processor.prepare(input: userInput)
            let stream = try MLXLMCommon.generate(
                input: lmInput,
                parameters: parameter.parameters,
                context: context
            )

            return .init { continuation in
                let task = Task {
                    for await generated in stream {
                        continuation.yield(generated.chunk ?? "")
                    }
                    continuation.finish()
                }
                continuation.onTermination = { _ in
                    task.cancel()
                }
            }
        }
    }
    
    /// Generates tool calls from the given input using default streaming implementation
    public func generateToolCalls(from input: LLMInput) async throws -> GeneratedContent {
        var text = ""
        var toolCalls: [LLMToolCall] = []
        
        for try await content in try await responseStream(from: input) {
            switch content {
            case .text(let chunk):
                text += chunk
            case .toolCall(let toolCall):
                toolCalls.append(toolCall)
            }
        }
        
        return GeneratedContent(text: text, toolCalls: toolCalls)
    }
    
    /// Resumes a conversation with tool outputs
    ///
    /// - Parameters:
    ///   - toolCalls: The tool calls that were made
    ///   - toolOutputs: The outputs from executing the tools (toolCallID, output)
    ///   - originalInput: The original input that generated the tool call
    /// - Returns: The model's response to the tool outputs
    /// - Throws: An error if text generation fails
    public func resume(
        withToolCalls toolCalls: [LLMToolCall],
        toolOutputs: [(String, String)],
        originalInput: LLMInput
    ) async throws -> String {
        guard case let .chat(messages) = originalInput.value else {
            throw LLMError.invalidParameter(reason: "Original input must be a chat")
        }
        
        var updatedMessages = messages

        // Add tool messages for each tool output
        for (toolCallID, output) in toolOutputs {
            updatedMessages.append(.tool(output, toolCallID: toolCallID))
        }
        
        // Create a new input with the updated messages
        let updatedInput = LLMInput.chat(updatedMessages)
        
        // Generate a response to the tool outputs
        return try await generateText(from: updatedInput)
    }
    
    /// Converts tool parameters to MLX format
    func convertParametersToMLXFormat(_ parameters: [String: any Sendable]) -> [ToolParameter] {
        guard let properties = parameters["properties"] as? [String: [String: any Sendable]] else {
            return []
        }
        
        let required = parameters["required"] as? [String] ?? []
        
        return properties.compactMap { key, value in
            guard let type = value["type"] as? String,
                  let description = value["description"] as? String else {
                return nil
            }
            
            let mlxType: ToolParameterType
            switch type {
            case "string":
                mlxType = .string
            case "integer", "number":
                mlxType = .int
            case "boolean":
                mlxType = .bool
            case "array":
                // Check for items schema to determine element type
                if let items = value["items"] as? [String: any Sendable] {
                    if let itemType = items["type"] as? String {
                        let elementType: ToolParameterType
                        switch itemType {
                        case "string":
                            elementType = .string
                        case "integer":
                            elementType = .int
                        case "number":
                            elementType = .double
                        case "boolean":
                            elementType = .bool
                        case "object":
                            // For array of objects, parse the object schema
                            if let objectProperties = items["properties"] as? [String: [String: any Sendable]] {
                                let mlxProperties = convertParametersToMLXFormat([
                                    "properties": objectProperties as [String: any Sendable],
                                    "required": (items["required"] as? [String] ?? []) as [String] as any Sendable
                                ])
                                elementType = .object(properties: mlxProperties)
                            } else {
                                elementType = .object(properties: [])
                            }
                        default:
                            elementType = .string
                        }
                        mlxType = .array(elementType: elementType)
                    } else {
                        mlxType = .array(elementType: .string) // Default to string array
                    }
                } else {
                    mlxType = .array(elementType: .string) // Default to string array
                }
            case "object":
                // Parse object properties if available
                if let objectProperties = value["properties"] as? [String: [String: any Sendable]] {
                    let mlxProperties = convertParametersToMLXFormat([
                        "properties": objectProperties as [String: any Sendable],
                        "required": (value["required"] as? [String] ?? []) as [String] as any Sendable
                    ])
                    mlxType = .object(properties: mlxProperties)
                } else {
                    mlxType = .object(properties: []) // Empty object
                }
            default:
                mlxType = .string
            }
            
            var extraProperties: [String: Any] = [:]
            if let enumValues = value["enum"] as? [String] {
                extraProperties["enum"] = enumValues
            }
            
            if required.contains(key) {
                return ToolParameter.required(key, type: mlxType, description: description, extraProperties: extraProperties.isEmpty ? [:] : extraProperties)
            } else {
                return ToolParameter.optional(key, type: mlxType, description: description, extraProperties: extraProperties.isEmpty ? [:] : extraProperties)
            }
        }
    }
    
    /// Streams responses from the input
    /// - Parameter input: The input to process
    /// - Returns: An asynchronous sequence that emits response content (text chunks, tool calls, etc.)
    /// - Throws: An error if generation fails
    public func responseStream(from input: LLMInput) async throws -> AsyncThrowingStream<StreamingChunk, Error> {
        return AsyncThrowingStream { continuation in
            Task {
                do {
                    // Convert tools to MLX schema format
                    let toolSchemas = tools.map { tool in
                        Tool<EmptyInput, EmptyOutput>(
                            name: tool.name,
                            description: tool.description,
                            parameters: convertParametersToMLXFormat(tool.argumentsSchema)
                        ) { _ in EmptyOutput() }.schema
                    }
                    
                    // Create chat messages from input
                    let chat = input.chatMessages
                    
                    // Create UserInput with tools
                    var userInput = UserInput(
                        chat: chat,
                        tools: toolSchemas.isEmpty ? nil : toolSchemas,
                        additionalContext: ["enable_thinking": false]
                    )
                    userInput.processing.resize = CGSize(width: 448, height: 448)
                    
                    if chat.contains(where: { !$0.images.isEmpty }), !context.supportsVision {
                        throw LLMError.visionUnsupported
                    }
                    
                    let modelContainer = context.modelContainer
                    
                    let stream = try await modelContainer.perform { [userInput] (context: ModelContext) -> AsyncStream<Generation> in
                        let lmInput = try await context.processor.prepare(input: userInput)
                        return try MLXLMCommon.generate(
                            input: lmInput,
                            parameters: parameter.parameters,
                            context: context
                        )
                    }
                    
                    for try await generation in stream {
                        switch generation {
                        case let .chunk(text):
                            continuation.yield(.text(text))
                        case .info: ()
                        case let .toolCall(toolCall):
                            // Convert MLX ToolCall to LLMToolCall
                            let arguments = try JSONEncoder().encode(toolCall.function.arguments)

                            let llmToolCall = LLMToolCall(
                                id: UUID().uuidString, // Generate ID since MLX ToolCall doesn't have one
                                name: toolCall.function.name,
                                arguments: String(decoding: arguments, as: UTF8.self)
                            )
                            continuation.yield(.toolCall(llmToolCall))
                        }
                    }
                    
                    continuation.finish()
                } catch {
                    continuation.finish(throwing: error)
                }
            }
        }
    }
}

// Empty types for parameter conversion
private struct EmptyInput: Codable {}
private struct EmptyOutput: Codable {}


public extension LocalLLMClient {
    /// Creates a new MLX client.
    ///
    /// This is a factory method for creating `MLXClient` instances.
    ///
    /// - Parameters:
    ///   - url: The URL of the MLX model directory. This directory should contain the model weights, tokenizer configuration, and any other necessary model files.
    ///   - parameter: The parameters for the MLX model. Defaults to `.default`.
    ///   - tools: Optional array of tools that can be used by the model for function calling.
    /// - Returns: A new `MLXClient` instance.
    /// - Throws: An error if the client fails to initialize, for example, if the model files cannot be loaded.
    static func mlx(url: URL, parameter: MLXClient.Parameter = .default, tools: [any LLMTool] = []) async throws -> MLXClient {
        try await MLXClient(url: url, parameter: parameter, tools: tools)
    }
}
