import Foundation
import LocalLLMClientCore

#if BUILD_DOCC
@preconcurrency @_implementationOnly import llama
#elseif canImport(llama)
@preconcurrency private import llama
#else
@preconcurrency import LocalLLMClientLlamaC
#endif

/// A utility to parse tool calls from a response generated by a model using llama.cpp's common_chat_parse
struct LlamaToolCallParser {
    /// Parses a string for tool calls with a specific chat format
    ///
    /// - Parameters:
    ///   - response: The string to parse for tool calls
    ///   - format: The specific chat format to use for parsing
    /// - Returns: An array of LLMToolCall objects if any were found, otherwise nil
    public static func parseToolCalls(from response: String, format: common_chat_format) -> [LLMToolCall]? {
        var syntax = common_chat_syntax()
        syntax.format = format
        syntax.reasoning_format = COMMON_REASONING_FORMAT_NONE
        syntax.reasoning_in_content = false
        syntax.thinking_forced_open = false
        syntax.parse_tool_calls = true
        
        let parsedMessage = common_chat_parse(std.string(response), false, syntax)
        guard !parsedMessage.tool_calls.empty() else {
            return nil
        }
        
        var toolCalls: [LLMToolCall] = []
        
        for i in 0..<parsedMessage.tool_calls.size() {
            let cppToolCall = parsedMessage.tool_calls[i]
            
            let id: String
            if cppToolCall.id.empty() {
                id = UUID().uuidString
            } else {
                id = String(cppToolCall.id)
            }
            
            let name = String(cppToolCall.name)
            let arguments = String(cppToolCall.arguments)
            
            let toolCall = LLMToolCall(
                id: id,
                name: name,
                arguments: arguments
            )
            
            toolCalls.append(toolCall)
        }
        
        return toolCalls.isEmpty ? nil : toolCalls
    }
}
